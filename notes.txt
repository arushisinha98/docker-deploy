## Notes from Udemy / learn.gov.sg: Docker Crash Course for busy DevOps and Developers
Docker = container-based virtualization
- container engine allows us to run multiple guest instances
- containers share the host's OS
- applications run in different containers (runtime isolation)
- containers are self-sufficient application bundles

Docker uses client-server architecture
- docker client is the primary UI
  - command line client ($)
- docker daemon / engine / server
- docker client + daemon run on the same host

Docker for Windows
#### $ docker info
(check that it is running on local Linux)

Images = read-only template used to create containers
- created with docker build command
- composed of layers w/ other images
- stored in docker registry (e.g. DockerHub)

Container = instance of a class, if image is a class
- runtime object
- portable encapsulations of an environment in which to run apps
- created from images
- contains all the binaries / dependencies needd to run app

Registry = where images are stored
- public or own
- images stored in repositories within a registry

Repository = collection of different docker images w/ same name, different tags
- each tag usually represents different version of the image

use official images from hub.docker.com -- clear documentation

All changes made into the running containers are written into the writable layer
When a container is deleted, writable layer is also deleted but underlying image remains unchanged
Multiple layers can share access to the same underlying image

##### Task 1: Create a container from an image on DockerHub
- search for busybox from hub.docker.com (tag: 1.24)
- open command prompt
#### $ docker images
#### $ docker run busybox:1.24 echo "hello world"
- docker run command will create the container using the image specified then will spin up the container and run it

##### Task 2: Github pull and push
#### $ docker run --name repo alpine/git clone https://github.com/docker/getting-started.git
#### $ docker cp repo:/git/getting-started/ .
#### $ cd getting-started
#### $ docker build -t docker101tutorial .
#### $ docker run -dp 80:80 --name docker-tutorial docker101tutorial
#### $ docker tag docker101tutorial arushisinha98/docker101tutorial
#### $ docker push arushisinha98/docker101tutorial

##### Task 3: Editing in container runtime
#### $ docker run busybox:1.24 ls /
#### $ docker run -it busybox:1.24
#### $ # ls
#### $ # touch a.txt
#### $ # exit
- created a file a.txt then exited container

##### Task 4: Display container information
#### $ docker ps
- displays container information
#### $ docker ps -a
- displays all containers including previous runs
#### $ docker run --rm busybox:1.24 sleep 1
- removes container as soon as it runs for 1 second
#### $ docker run --name hello_world busybox:1.24
- specifies container name as hello_world
#### $ docker inspect hello_world

#### $ docker pull python
#### $ docker run -it -p 8888:8080 tomcat:8.0
- view https://localhost:8888
#### $ docker ps -a
#### $ docker run -it -dp 8888:8080 tomcat:8.0
#### $ docker run -itdp 8888:8080 --name hello_world2 tomcat:8.0
#### $ docker logs [LONG_CONTAINER_ID]

#### $ docker history busybox:1.24

##### Task 5: Commit changes in a container
- spin up a container from a base image
- install git package in container
- commit changes in container
#### $ docker run -it debian:jessie
#### $ # ls
#### $ # git
#### $ # apt-get update && apt-get install -y git
#### $ # git
#### $ # exit
#### $ docker ps -a
#### $ docker commit [CONTAINER_ID] arushisinha98/debian:1.00
#### $ docker images
#### $ docker run -it arushisinha98/debian:1.00
#### $ # ls
#### $ # git

##### Task 6: Write a Dockerfule to build a docker image
- dockerfile = a .txt document that contains all the instructions users provide to assemble an image
- each instruction will create a new image layer to the image
- instructions specity what to do when building the image

#### $ wsl touch Dockerfile
#### $ wsl vi Dockerfile

#### FROM debian:jessie
#### RUN apt-get update
#### RUN apt-get install -y git
#### RUN apt-get install -y vim

#### $ docker build -r arushisinha98/debian .
- docker build command takes the path to the build context as an argument
- when build starts, docker client packs all files into tarball then transfers tarball to the daemon
- "." used to denote current directory
- docker creates a new container for each instruction
- containers are ephemeral and are only used to create images
- containers are read-only

Dockerfile syntax and best practices
- reach RUN command will execute the command on the top writable layer of the container then commit container as new image
- each new image is used for the next step in Dockerfile
- so each RUN instruction will create a new image layer
- recommend to chain the RUN instructions in Dockerfile to reduce the number of image layers created
- sort multi-line arguments alphanumerically to avoid duplication and easy update
- CMD instructions specify which command to run when container starts up
- COPY instuction copies new files or directories from build context and adds tehm to the file system of the container
- ADD instruction can not only copy files but also download files from teh internet
- ADD can automatically unpack compressed files

#### # wsl vi Dockerfile

#### FROM debian:jessie
#### RUN apt-get update && apt-get install -y \
#### git \
#### python \
#### vim \
#### COPY abc.txt /src/abc.txt
#### CMD ["echo", "hello world"]

#### $ docker build -t arushisinha98/debian .
#### $ docker run -it arushisinha98/debian
#### $ docker run [IMAGE_ID] echo "hello docker"
- overwrites CMD instruction

- each time docker executes an instruction, it builds a new image layer
- if instruction unchanged, it uses docker cache to expedite

#### $ docker build -t arushisinha98/debian . --no-cache=true
- chain to change the first instruction or specify no cache

- docker uses "latest" when no tag is provided
- images tagged "latest" will not be updated automatically when newwer version of the image is pushed to the repository
- avoid using "latest" tag

#### $ docker images
#### $ docker tag [IMAGE_ID] arushisinha98/debian:1.01
#### $ docker images
#### $ docker login --username=arushisinha98
#### [ENTER PASSWORD]
#### $ docker push arushisinha98/debian:1.01

##### Task 7: Containerize a hello world web application
- Flask is a lightweight web framework
- download and install git

#### $ git clone -b v0.1 https://github.com/jleetutorial/dockerapp.git
#### $ wsl ls
#### $ cd dockerapp
#### $ wsl ls
#### $ cd app
#### $ wsl ls
#### $ wsl vi app.py

#### from flask import Flask
#### app = Flask(__name__)
#### # import Flask, initialize Flask, create app object
#### @app.route('/')
#### def hello_world():
####  return "Hello World!"
#### if __name__ = '__main__':
####  app.run(host = '0.0.0.0')

- register a view function for a given URL
- initializes and runs the Python Flask server
- do not use this run method to start an app in production
- this is NOT intended to meet secutiry and performance requirements for production server
- instead, use UWSGI or CGI (Google Flask deployment options for more info)
- using app server at 0.0.0.0 instead of local host or 127.0.0.1
- needed if you want to have server available externally (app server can be accessed by other containers)

#### $ cd ..
#### $ wsl vi Dockerfile

#### FROM python-3.5
#### RUN pip install Flask==0.11.1
#### RUN useradd -ms /bin/bash admin
#### USER admin
#### WORKDIR /app
#### COPY app /app
#### CMD ["python", "app.py"]

- use non-privileged user for better security

#### $ docker build -t dockerapp:v0.1 .
#### $ docker images
#### $ docker run -dp 5000:5000 [IMAGE_ID]

https://localhost:5000

#### $ docker ps -a
#### $ docker exec -it [CONTAINER_ID] bash
#### $ $ pwd
#### $ $ cd/home/admin/
#### $ $ ps axu
#### $ $ exit

#### $ git stash && git checkout v0.2
#### $ subl .
- open Sublime Text (download & install beforehand)
#### $ docker build -t dockerapp:v0.2 .
#### $ docker run -dp 5000:5000 dockerapp:v0.2

https://localhost:5000

##### Task 8: Linking containers

Redis is an in-memory data structure store
- used as a database, cache, message broker
- built-in replication and different levels of on-disk persistence

Link redis and dockerapp containers
- dockerapp is the source >> redis is the receiver
Docker container links
- when we build an app with a microservice architecture, we are able to run many indepenent components in different containers
- docker creates a secure tunnel b/w containers that doesn't need to expose any ports externally on the container

Make edits to app.py:
from flask import Flask, request, render_template
import redis

app = Flask(__name__)
default_key = '1'
cache = redis.StrictRedis(host = 'redis', port = 6379, db = 0)
cache.set(default_key, "one")

@app.route('/', methods=['GET', 'POST'])
def mainpage():

	key = default_key
	if 'key' in request.form:
	    key = request.form['key']

	if request.method == 'POST' and request.form['submit'] == 'save':
		cache.set(key, request.form['cache_value'])

	cache_value = None;
	if cache.get(key):
		cache_value = cache.get(key).decode('utf-8')

	return render_template('index.html', key = key, cache_value = cache_value)

if __name__ == '__main__':
    app.run(host='0.0.0.0')

Make edits to Dockerfile:
FROM python:3.5
RUN pip install Flask==0.11.1 redis==2.10.5
RUN useradd -ms /bin/bash admin
USER admin
WORKDIR /app
COPY app /app
CMD ["python", "app.py"]

#### $ docker run -d --name redis redis:3.2.0
#### $ docker ps
#### $ docker build -t dockerapp:v0.3 .
#### $ docker run -dp 5000:5000 --link redis dockerapp:v0.3
#### $ docker ps
#### $ docker exec -it [CONTAINER_ID] bash
#### $ $ more /etc/hosts
#### $ $ exit
#### $ docker inpsect [CONTAINER_ID] | wsl grep IP
#### $ docker exec -it [CONTAINER_ID] bash
#### $ $ ping redis

start redis container >> start dockerapp container >> link redis container
- this process is impractical as the number of containers grows

##### Task 9: Docker compose
Docker compose is a tool for defining and running multiple docker applications
docker-compose.yml file to start all the containers
- stores configuration of all the containers
- removes burden to maintain scripts for docker orchestration

#### $ docker-compose version
#### $ cd dockerapp
#### $ wsl ls
#### $ wsl touch docker-compose.yml
#### $ subl .

Contents of docker-compose.yml file:
version: '3'
services:
  dockerapp:
    build: . # path to the Dockerfile ("." = current directory)
    ports:
    - "5000:5000"
    depends_on:
    - redis # dockerapp is a client of the redis container
  redis:
    image: redis:3.2.0
# container linking not required in this version

#### $ docker ps
#### $ docker stop [CONTAINER_ID1 CONTAINER_ID2 ...]
#### $ docker-compose up

#### $ cd dockerapp
#### $ git stash && git checkout v0.4

Other docker-compose commands:
#### $ docker-compose up (starts up all the containers)
#### $ docker-compose up -d (to run in the background)
#### $ docker-compose ps (outputs status of containers managed by docker-compose)
#### $ docker-compose logs [-f] (outputs colored and aggregated logs for the compose-managed containers; -f outputs appended logs when the logs grow)
#### $ docker-compose logs dockerapp (outputs logs of a specific container, dockerapp in this case)
#### $ docker-compose stop (stops all the running containers w/o removing them)
#### $ docker-compose rm [y/N] (removes all containers)
#### $ docker-compose build (rebuilds all the images)

#### $ wsl vi Dockerfile
(change user to James)
#### $ docker-compose up
#### $ docker ps
#### $ docker exec -it [CONTAINER_ID] bash
(logs in as admin, not James)
#### $ # exit
#### $ cd dockerapp
#### $ docker-compose build
(rebuilds image)
#### $ docker-compose up
#### $ docker ps
#### $ docker exec -it [CONTAINER_ID] bash
(logs in as James)

Best practices:
Each container should have only one specific targeted process
- easy to scale
- better reusability
- easy to troubleshoot
Save data on volumes
- safely shared among multiple containers
- does not increase the size of the container
- data exists outside the life cycle of a given container
Use names or environment variables to pass information b/w containers
- IP address changes when containers start and stop
Run as normal user, not privileged
Keep containers small

##### Task 10: Docker container networking
Container A + Container B <==> Bridge (docker0) -- host <==> internet

Closed network (none network)
- no access to outside world
- isolated containers
- maximum level of network protection
- not a good choice if internet connection is required

Bridge network
- containers have access to loopback and private interface
- all containers in the same bridge network can communicate w/ each other
- containers from different bridge networks cannot connect w/ each other by default
- reduces level of network isolation in favor of better outside connectivity
- most suitable where you want to set up a relatively small network on a single host

Host network
- least protected network model
- adds a container on the host's network stack
- containers deployed on the host stack have full access to host's interface (open containers)
- no isolation, leaving containers widely unprotected
- containers running in host network stack see higher level of performance than those traversing the docker0 bridge and iptables port mappings

Overlay network
- supports multi-host networking out of the box
- require some pre-existing conditions before it can be created
	running docker engine in swarm mode (Google: create a swarm cluster)
	a key-valye store such as consul
- widely used in production

#### $ docker network ls
#### $ docker run -d --net none busybox sleep 1000
#### $ docker exec -it [LONG_CONTAINER_ID] /bin/ash
#### /# ping 8.8.8.8
(Google IP unreachable)
#### /# ifconfig
#### /# exit
#### $ ping 8.8.8.8
(Google IP reachable)

#### $ docker network inspect bridge
(IP range for bridge network: 172.17.0.0 - 172.17.255.255)
#### $ docker run -d --name container_1 busybox sleep 1000
#### $ docker exec -it container_1 ifconfig
#### $ docker run -d --name container_2 busybox sleep 1000
#### $ docker exec -it container_2 ifconfig
#### $ docker exec -it container_1 ping [IP_FROM_ABOVE]
(can ping each other via private IP interfaces of each other)
#### $ docker exec -it container_1 ping 8.8.8.8

#### $ docker network create --driver bridge my_bridge
#### $ docker network ls
#### $ docker network inspect my_bridge
#### $ docker run -d --name container_3 --net my_bridge busybox sleep 1000
#### $ docker exec -it container_3 ifconfig
#### $ docker exec -it container_3 ping [IP_OF_CONTAINER_1]

#### $ docker network connect bridge container_3
#### $ docker exec -it container_3 ifconfig
(now able to ping container_1)
#### $ docker network disconnect bridge container_3

#### $ docker run -d --name container_4 --net host busybox sleep 1000
#### $ docker exec -it container_4 ifconfig

##### Task 11: Define container networks with docker compose

#### $ git stash && git checkout v0.4
#### $ docker-compose up
#### $ docker network ls
#### $ docker-compose down

Make edits to docker-compose.yml:
version: '3'
services:
  dockerapp:
    build: .
    ports:
      - "5000:5000"
    depends_on:
      - redis
    networks:
      - my_net
  redis:
    image: redis:3.2.0
    networks:
      - my_net
networks:
  my_net:
    driver: bridge
    
#### $ docker-compose up

More complex network isolation:

docker-compose.yml
version: '2'
services:
  proxy:
    build: ./proxy
    networks:
      - front
  app:
    build: ./app
    networks:
      - front
      - back
  db:
    image: postgres
    networks:
      - back
networks:
  front:
    # use a custom driver
    driver: custom-driver-1
  back:
    # use a custom driver which takes special options
    driver: custom-driver-2
    driver_opts:
      foo: "1"
      bar: "2"

##### Task 12: Writing unit tests for app and running them inside a container
- unit tests should test basic functionality of dockerapp code w/o reliance on external services
- unit tests should run quickly so that developers can iterate quickly w/o waiting for results
- docker containers can spin up in seconds and can createa a clean and isolated environment to run unit tests

Python unit testing framework: unittest

##### Task 13: Continuous integration
Continuous integration = software engineering practice in which isolated changes are immediately tested and report when added to larger code base
- goal is to provide rapid feedback
- defect can be identified and corrected ASAP

- developer ==> commit changes to central repository
- continuous integration monitors central repository
- triggers a build (process varies but involves building app and running unit test)
- assign build label to the code that was built
- can be configured to deploy the app in production server

- CI server pushes image to docker registry
- pull docker images
- development environment

Github as central repository for version control
Circle CI is a hosted CI server

- fork https://github.com/jleetutorial/dockerapp
set up SSH keys for Github
- SSH keys are a way to identify trusted computers w/o password
- generate SSH key pair and save private SSH key in local box
- add public key to Github account
- directly push changes to Github repository
SSH public key file sits under ~/.ssh/ directory and ends with .pub

#### $ ssh-keygen -t rsa -b 4096 -C "arushi_sinha@tech.gov.sg"
[ENTER]
no passphrase [ENTER] x 2
Start >> windows administrative tools >> computer management >> services and applications >> services >> OpenSSH authentication agent
Startup type = Manual >> apply >> OK
#### $ eval "$(ssh-agent -s)" (start SSH agent then add key to agent)
#### $ ssh-add ~/.ssh/id_rsa
#### $ pbcopy < ~./ssh/id_rsa.pub (now can add to Github)

Github profile >> settings >> new SSH key >> give a title >> paste public key >> add
------------------------------------------------------------------------------------------
wsl.exe --shutdown
wsl.exe
